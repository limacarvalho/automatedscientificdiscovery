import logging
from typing import Union

import numpy as np
import pandas as pd
from scipy.spatial.distance import cdist
from sklearn.base import BaseEstimator
from utils_logger import LoggerSetup

# Initialize logging object (Singleton class) if not already
LoggerSetup()


class Sammon(BaseEstimator):
    """Python Sammon mapping
    Simple python implementation of Sammon's non-linear mapping algorithm.
    We thank author Tom Pollard for the implementation of this algorithm according to (1).
    source: https://github.com/tompollard/sammon (with small modifications

    (1) Sammon, John W. Jr., "A Nonlinear Mapping for Data Structure Analysis",
    IEEE Transactions on Computers, vol. C-18, no. 5, pp 401-409, May 1969.

    we adapted it to the sklearn and our nomenclature and added some sklearn functionality for seamless
    integration into our pipeline.
    """

    def __init__(
        self,
        n_components: Union[int, None],
        inputdist: str = "raw",
        maxhalves: int = 20,
        max_iter: int = 500,
        tolfun: float = 1e-9,
        init: str = "default",
    ):
        self.n_components = n_components
        self.inputdist = inputdist
        self.maxhalves = maxhalves
        self.max_iter = max_iter
        self.tolfun = tolfun
        self.init = init
        self.display = 2

    def fit_transform(self, data_high: Union[pd.DataFrame, np.array]):
        """
        - - - INFORMATION - - -
        Perform Sammon mapping on dataset x
        y = sammon(x) applies the Sammon nonlinear mapping procedure on
        multivariate data x, where each row represents a pattern and each column
        represents a feature.  On completion, y contains the corresponding
        co-ordinates of each point on the map.  By default, a two-dimensional
        map is created.  Note if x contains any duplicated rows, SAMMON will
        fail (ungracefully).
        [y,E] = sammon(x) also returns the value of the cost function in E (i.e.
        the stress of the mapping).
        An N-dimensional output map is generated by y = sammon(x,n) .
        A set of optimisation options can be specified using optional

        - - - PARAMETERS - - -
        arguments, y = sammon(x,n,[OPTS]):
           maxiter        - maximum number of iterations
           tolfun         - relative tolerance on objective function default: 1e-9
           maxhalves      - maximum number of step halvings
           inputdist      - {'raw','distance'} if set to 'distance', X is
                            interpreted as a matrix of pairwise distances.
           display        - 0 to 2. 0 least verbose, 2 max verbose.
           init           - {'pca', 'cmdscale', random', 'default'}
                            default is 'pca' if input is 'raw',
                            'msdcale' if input is 'distance'
        The default options are retrieved by calling sammon(x) with no
        parameters.

        """
        # Create distance matrix unless given by parameters
        # We only use inputdist = 'raw' and init = pca, therefore we dont use this part
        # if self.inputdist == 'distance':
        #     D = data_high
        #     if self.init == 'default':
        #         self.init = 'cmdscale'
        # else:
        #     D = cdist(data_high, data_high)
        #     if self.init == 'default':
        #         self.init = 'pca'
        #
        # if self.inputdist == 'distance' and self.init == 'pca':
        #     logging.error("Cannot use init == 'pca' when inputdist == 'distance'")

        D = cdist(data_high, data_high)
        if np.count_nonzero(np.diagonal(D)) > 0:
            logging.error("The diagonal of the dissimilarity matrix must be zero")

        # Remaining initialisation
        N = data_high.shape[0]
        D = D + np.eye(N)

        if np.count_nonzero(D <= 0) > 0:
            logging.error("Off-diagonal dissimilarities must be strictly positive")

        Dinv = 1 / D
        if self.init == "pca":
            [UU, DD, _] = np.linalg.svd(data_high)
            y = UU[:, : self.n_components] * DD[: self.n_components]

        # We only use inputdist = 'raw' and init = pca, therefore we dont use this part
        # elif self.init == 'cmdscale':
        #     from .py_cmdscale import cmdscale
        #     y ,e = cmdscale(D)
        #     y = y[: ,:self.n_components]

        else:
            y = np.random.normal(0.0, 1.0, [N, self.n_components])
        one = np.ones([N, self.n_components])
        d = cdist(y, y) + np.eye(N)
        dinv = 1.0 / d
        delta = D - d
        E = ((delta**2) * Dinv).sum()

        # Get on with it
        for i in range(self.max_iter):
            # Compute gradient, Hessian and search direction (note it is actually
            # 1/4 of the gradient and Hessian, but the step size is just the ratio
            # of the gradient and the diagonal of the Hessian so it doesn't
            # matter).
            delta = dinv - Dinv
            deltaone = np.dot(delta, one)
            g = np.dot(delta, y) - (y * deltaone)
            dinv3 = dinv**3
            y2 = y**2
            H = np.dot(dinv3, y2) - deltaone - np.dot(2, y) * np.dot(dinv3, y) + y2 * np.dot(dinv3, one)
            s = -g.flatten(order="F") / np.abs(H.flatten(order="F"))
            y_old = y

            # Use step-halving procedure to ensure progress is made
            for j in range(self.maxhalves):
                s_reshape = np.reshape(s, (-1, self.n_components), order="F")
                y = y_old + s_reshape
                d = cdist(y, y) + np.eye(N)
                dinv = 1 / d
                delta = D - d
                E_new = ((delta**2) * Dinv).sum()
                if E_new < E:
                    break
                else:
                    s = 0.5 * s

                # Bomb out if too many halving steps are required
                if j == self.maxhalves - 1:
                    logging.info("py_sammon, Warning: maxhalves exceeded. Sammon mapping may not converge...")

            # Evaluate termination criterion
            if abs((E - E_new) / E) < self.tolfun:
                break

        return y
