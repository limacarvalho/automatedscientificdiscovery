{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..') # add parent folder path where lib folder is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import dasker, config as util_config\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from keras import models, layers\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.backend import clear_session\n",
    "\n",
    "import warnings\n",
    "\n",
    "import optuna\n",
    "import dask_optuna\n",
    "from ml.preprocess.data import SingletonDataSet\n",
    "# from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import dask.dataframe as ddf\n",
    "from dask import distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/rwmas/GitHub/xai/xai_api/app/notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../test/data/20220319_covid_merge_processed.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_dataset():\n",
    "    df = pd.read_csv('../test/data/20220319_covid_merge_processed.csv', sep=\",\")\n",
    "\n",
    "    dataset = SingletonDataSet()\n",
    "    dataset.load_data(df, 0, 0, 'regression')\n",
    "    X, y = dataset.get_data()\n",
    "    return X, y\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_dataset():\n",
    "    df = pd.read_csv('../test/data/20220319_covid_merge_processed.csv', sep=\",\")\n",
    "\n",
    "    dataset = SingletonDataSet()\n",
    "    dataset.load_data(df, 0, 0, 'regression')\n",
    "    X, y = dataset.get_data()\n",
    "    return X, y\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_shallow_model(trial, input_dim, lr, loss, pred_type):\n",
    "\n",
    "    model = models.Sequential()\n",
    "    # Input layer\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    hidden_units = []\n",
    "\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        n_units = trial.suggest_int(\"n_units_l{}\".format(i), 1, 128)\n",
    "        # hidden_units.append(n_units)\n",
    "        # Input layer\n",
    "        model.add(layers.Dense(n_units, name = \"dense_hidden_\"+str(i), kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "    #model.add(layers.Dense(32, name = \"dense_hidden\", input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "        \n",
    "    # model.add(layers.Dense(8, name = \"out\", activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    if pred_type == 'classification':\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    else:\n",
    "        model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "    # Compile a model\n",
    "    model.compile(loss=loss, optimizer=Adam(lr), metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "def objective_dask_array(trial):\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    clear_session()\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    df_X, df_y = get_dataset()\n",
    "\n",
    "    df_X = StandardScaler().fit_transform(df_X)\n",
    "\n",
    "\n",
    "    X = ddf.from_pandas(df_X, npartitions=no_of_workers)\n",
    "    y = ddf.from_pandas(df_y, npartitions=no_of_workers)\n",
    "\n",
    "    X = X.to_dask_array(lengths=True)\n",
    "    y = y.to_dask_array(lengths=True)\n",
    "\n",
    "\n",
    "    X_base, X_val, y_base, y_val = train_test_split(X, y, random_state=0, shuffle=True)\n",
    "    \n",
    "    input_shape = X_base.shape[1]\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    loss = 'mean_squared_error'\n",
    "\n",
    "    model = build_shallow_model(trial, input_shape, learning_rate, loss, 'regression')\n",
    "\n",
    "\n",
    "    if pred_type == 'classification':\n",
    "        cv = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "    else:\n",
    "        cv = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X_base, y_base):\n",
    "        X_train, y_train = X_base[train_idx], y_base[train_idx]\n",
    "        X_test, y_test = X_base[test_idx], y_base[test_idx]\n",
    "\n",
    "        model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                # validation_data=(X_val, y_val),        \n",
    "                shuffle=True,\n",
    "                batch_size=BATCHSIZE,\n",
    "                epochs=EPOCHS,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "        # Evaluate the model accuracy on the validation set.\n",
    "        # score = model.evaluate(X_val, y_val, verbose=0)\n",
    "        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "        scores.append(score[1])\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        error = mean_squared_error(y_test, y_pred)\n",
    "                \n",
    "    print(np.mean(error))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    #score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    # return score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective_cv(trial):\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    clear_session()\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    df_X, df_y = get_dataset()\n",
    "\n",
    "    df_X = StandardScaler().fit_transform(df_X)\n",
    "\n",
    "    X_base, X_val, y_base, y_val = train_test_split(df_X, df_y, random_state=0, shuffle=True)\n",
    "    \n",
    "    input_shape = X_base.shape[1]\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    loss = 'mean_absolute_error'\n",
    "\n",
    "    model = build_shallow_model(trial, input_shape, learning_rate, loss, 'regression')\n",
    "\n",
    "    if pred_type == 'classification':\n",
    "        cv = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "    else:\n",
    "        cv = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X_base, y_base):\n",
    "        X_train, X_test = X_base.loc[X_base.index[train_idx]], X_base.loc[X_base.index[test_idx]]\n",
    "        y_train, y_test = y_base.loc[y_base.index[train_idx]], y_base.loc[y_base.index[test_idx]]\n",
    "        \n",
    "        model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                # validation_data=(X_val, y_val),        \n",
    "                shuffle=True,\n",
    "                batch_size=BATCHSIZE,\n",
    "                epochs=EPOCHS,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "        # Evaluate the model accuracy on the validation set.\n",
    "        # score = model.evaluate(X_val, y_val, verbose=0)\n",
    "        # score = model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        error = np.round_(mean_squared_error(y_test, y_pred), decimals=2, out=None)\n",
    "                \n",
    "        scores.append(error)\n",
    "\n",
    "        # scores.append(score[1])\n",
    "                \n",
    "    mean_score = np.mean(scores)\n",
    "\n",
    "    print('mean score:' + str(mean_score))\n",
    "\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    #score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCHSIZE = 128\n",
    "EPOCHS = 1000\n",
    "N_TRIALS = 50\n",
    "no_of_workers = 4\n",
    "\n",
    "pred_type = \"regression\"\n",
    "\n",
    "model_path = \"/mnt/c/Users/rwmas/GitHub/xai/xai_api/app/ml/models/saved/base/brisk_neural_net/\"\n",
    "# CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective(trial):\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    clear_session()\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    df_X, df_y = get_dataset()\n",
    "\n",
    "    df_X = StandardScaler().fit_transform(df_X)\n",
    "\n",
    "    X_base, X_val, y_base, y_val = train_test_split(df_X, df_y, random_state=0, shuffle=True)\n",
    "    \n",
    "    input_shape = X_base.shape[1]\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128, 512, 1024, 2048])\n",
    "\n",
    "    loss = 'mean_squared_error'\n",
    "\n",
    "    model = build_shallow_model(trial, input_shape, learning_rate, loss, 'regression')\n",
    "\n",
    "        \n",
    "    model.fit(\n",
    "            X_base,\n",
    "            y_base,\n",
    "            validation_data=(X_val, y_val),\n",
    "            shuffle=True,\n",
    "            batch_size=bs,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=False,\n",
    "        )\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    err = np.round_(mean_squared_error(y_val, y_pred), decimals=2, out=None)\n",
    "    \n",
    "    print(err)\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    #score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-15 09:00:34,081]\u001b[0m A new study created in memory with name: no-name-410f11e5-eacc-4cc0-8b26-796dc213272d\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask dashboard is available at http://127.0.0.1:8787/status\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "4159987.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-15 09:01:22,368]\u001b[0m Trial 0 finished with value: 4159987.17 and parameters: {'learning_rate': 1.2253155021127636e-05, 'batch_size': 512, 'n_layers': 2, 'n_units_l0': 4, 'n_units_l1': 39}. Best is trial 0 with value: 4159987.17.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "1130238.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-15 09:01:24,022]\u001b[0m Trial 1 finished with value: 1130238.2 and parameters: {'learning_rate': 0.0006761863357037314, 'batch_size': 512, 'n_layers': 3, 'n_units_l0': 19, 'n_units_l1': 33, 'n_units_l2': 53}. Best is trial 1 with value: 1130238.2.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "2474701.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-15 09:01:25,739]\u001b[0m Trial 2 finished with value: 2474701.92 and parameters: {'learning_rate': 0.0003028457276691092, 'batch_size': 128, 'n_layers': 1, 'n_units_l0': 22}. Best is trial 1 with value: 1130238.2.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "1591705.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-15 09:01:30,549]\u001b[0m Trial 4 finished with value: 1591705.15 and parameters: {'learning_rate': 0.004120790429825028, 'batch_size': 512, 'n_layers': 1, 'n_units_l0': 83}. Best is trial 1 with value: 1130238.2.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "1442725.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-15 09:01:36,168]\u001b[0m Trial 9 finished with value: 1442725.29 and parameters: {'learning_rate': 0.00032536514234802767, 'batch_size': 64, 'n_layers': 1, 'n_units_l0': 114}. Best is trial 1 with value: 1130238.2.\u001b[0m\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f19bffd9700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[32m[I 2022-09-15 09:01:37,163]\u001b[0m Trial 7 finished with value: 1020612.09 and parameters: {'learning_rate': 0.021259734643756672, 'batch_size': 64, 'n_layers': 3, 'n_units_l0': 101, 'n_units_l1': 62, 'n_units_l2': 30}. Best is trial 7 with value: 1020612.09.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "1020612.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2b58250f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[32m[I 2022-09-15 09:01:44,184]\u001b[0m Trial 5 finished with value: 1311320.06 and parameters: {'learning_rate': 0.03922372632682, 'batch_size': 32, 'n_layers': 3, 'n_units_l0': 26, 'n_units_l1': 86, 'n_units_l2': 61}. Best is trial 7 with value: 1020612.09.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "1311320.06\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "1070602.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-09-15 09:01:44,602]\u001b[0m Trial 3 finished with value: 1070602.17 and parameters: {'learning_rate': 0.00013807148520469757, 'batch_size': 16, 'n_layers': 3, 'n_units_l0': 50, 'n_units_l1': 6, 'n_units_l2': 59}. Best is trial 7 with value: 1020612.09.\u001b[0m\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2b7a0bbb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[32m[I 2022-09-15 09:01:49,610]\u001b[0m Trial 6 finished with value: 3896770.0 and parameters: {'learning_rate': 1.4680001680461255e-05, 'batch_size': 16, 'n_layers': 2, 'n_units_l0': 17, 'n_units_l1': 46}. Best is trial 7 with value: 1020612.09.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "3896770.0\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "1595869.44\n",
      "Number of trials: 10\n",
      "Best trial:\n",
      "  Value: 1020612.09\n",
      "  Params: \n",
      "    learning_rate: 0.021259734643756672\n",
      "    batch_size: 64\n",
      "    n_layers: 3\n",
      "    n_units_l0: 101\n",
      "    n_units_l1: 62\n",
      "    n_units_l2: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f19bff91040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[32m[I 2022-09-15 09:02:00,018]\u001b[0m Trial 8 finished with value: 1595869.44 and parameters: {'learning_rate': 0.00033344001803178435, 'batch_size': 1024, 'n_layers': 2, 'n_units_l0': 48, 'n_units_l1': 11}. Best is trial 7 with value: 1020612.09.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "import joblib\n",
    "\n",
    "# with Client() as client:\n",
    "client = dasker.get_dask_client()\n",
    "print(f\"Dask dashboard is available at {client.dashboard_link}\")\n",
    "\n",
    "storage = dask_optuna.DaskStorage()\n",
    "study = optuna.create_study(storage=storage, direction=\"minimize\")\n",
    "\n",
    "with joblib.parallel_backend(\"dask\"):\n",
    "    study.optimize(objective, n_trials=10, n_jobs=-1)\n",
    "\n",
    "print(\"Number of trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  10\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  10\n"
     ]
    }
   ],
   "source": [
    "pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.021259734643756672,\n",
       " 'batch_size': 64,\n",
       " 'n_layers': 3,\n",
       " 'n_units_l0': 101,\n",
       " 'n_units_l1': 62,\n",
       " 'n_units_l2': 30}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "\n",
    "loss = 'mean_squared_error'\n",
    "\n",
    "learning_rate = trial.params['learning_rate']\n",
    "\n",
    "batch_size = trial.params['batch_size']\n",
    "\n",
    "n_layers = trial.params['n_layers']\n",
    "units = []\n",
    "for param in trial.params:\n",
    "    if fnmatch.fnmatch(param, 'n_units*'):\n",
    "        # print(param)\n",
    "        units.append(param)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_model = models.Sequential()\n",
    "# Input layer\n",
    "\n",
    "best_model.add(layers.BatchNormalization())\n",
    "\n",
    "for i in range(n_layers):\n",
    "    best_model.add(layers.Dense(trial.params[units[i]], name = \"dense_hidden_\"+str(i), kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "best_model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "# Compile a model\n",
    "best_model.compile(loss=loss, optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, df_y = get_dataset()\n",
    "\n",
    "# df_X = StandardScaler().fit_transform(df_X)\n",
    "\n",
    "X_base, X_val, y_base, y_val = train_test_split(df_X, df_y, random_state=0, shuffle=True)\n",
    "\n",
    "input_shape = X_val.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ef43ba730>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "best_model.fit(\n",
    "        X_base,\n",
    "        y_base,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        epochs=300,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_1 (Batc  (None, 41)               164       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_hidden_0 (Dense)      (None, 101)               4242      \n",
      "                                                                 \n",
      " dense_hidden_1 (Dense)      (None, 62)                6324      \n",
      "                                                                 \n",
      " dense_hidden_2 (Dense)      (None, 30)                1890      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,651\n",
      "Trainable params: 12,569\n",
      "Non-trainable params: 82\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "x_val_pred = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4321093.345267444"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#err = \n",
    "mean_squared_error(y_val, x_val_pred)\n",
    "    \n",
    "# print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4321094.0, 0.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = best_model.evaluate(X_val, y_val, batch_size = batch_size, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder `my_model`.\n",
    "best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6ff7914d2bc405bfa61a835d0564ba294eb5f3f9b4c24865f45d9397593b123"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
